{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-11-20 15:26:32,755 word2vec.py[line:1567] INFO collecting all words and their counts\n",
      "2018-11-20 15:26:32,757 word2vec.py[line:1552] INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-20 15:26:32,758 word2vec.py[line:1575] INFO collected 15 word types from a corpus of 16 raw words and 2 sentences\n",
      "2018-11-20 15:26:32,758 word2vec.py[line:1626] INFO Loading a fresh vocabulary\n",
      "2018-11-20 15:26:32,762 word2vec.py[line:1650] INFO effective_min_count=1 retains 15 unique words (100% of original 15, drops 0)\n",
      "2018-11-20 15:26:32,764 word2vec.py[line:1656] INFO effective_min_count=1 leaves 16 word corpus (100% of original 16, drops 0)\n",
      "2018-11-20 15:26:32,768 word2vec.py[line:1715] INFO deleting the raw counts dictionary of 15 items\n",
      "2018-11-20 15:26:32,770 word2vec.py[line:1718] INFO sample=0.001 downsamples 15 most-common words\n",
      "2018-11-20 15:26:32,774 word2vec.py[line:1721] INFO downsampling leaves estimated 2 word corpus (13.7% of prior 16)\n",
      "2018-11-20 15:26:32,779 base_any2vec.py[line:1022] INFO estimated required memory for 15 words and 100 dimensions: 19500 bytes\n",
      "2018-11-20 15:26:32,780 word2vec.py[line:1834] INFO resetting layer weights\n",
      "2018-11-20 15:26:32,782 base_any2vec.py[line:1210] INFO training model with 3 workers on 15 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-20 15:26:32,792 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 15:26:32,796 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 15:26:32,798 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 15:26:32,800 base_any2vec.py[line:1346] INFO EPOCH - 1 : training on 16 raw words (2 effective words) took 0.0s, 237 effective words/s\n",
      "2018-11-20 15:26:32,868 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 15:26:32,872 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 15:26:32,874 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 15:26:32,876 base_any2vec.py[line:1346] INFO EPOCH - 2 : training on 16 raw words (3 effective words) took 0.0s, 369 effective words/s\n",
      "2018-11-20 15:26:32,893 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 15:26:32,895 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 15:26:32,897 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 15:26:32,902 base_any2vec.py[line:1346] INFO EPOCH - 3 : training on 16 raw words (1 effective words) took 0.0s, 106 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 15:26:32,944 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 15:26:32,946 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 15:26:32,948 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 15:26:32,955 base_any2vec.py[line:1346] INFO EPOCH - 4 : training on 16 raw words (2 effective words) took 0.0s, 170 effective words/s\n",
      "2018-11-20 15:26:32,973 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-20 15:26:32,974 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-20 15:26:32,975 base_any2vec.py[line:349] INFO worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-20 15:26:32,976 base_any2vec.py[line:1346] INFO EPOCH - 5 : training on 16 raw words (2 effective words) took 0.0s, 451 effective words/s\n",
      "2018-11-20 15:26:32,981 base_any2vec.py[line:1382] INFO training on a 80 raw words (10 effective words) took 0.2s, 51 effective words/s\n",
      "2018-11-20 15:26:32,983 base_any2vec.py[line:1386] WARNING under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "d:\\python\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "d:\\python\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.056299783"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', level=logging.INFO)\n",
    "\n",
    "# 引入数据集\n",
    "sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\n",
    "\n",
    "# 切分词汇\n",
    "vocab = [s.split() for s in sentences]\n",
    "print(vocab)\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(vocab, min_count=1)\n",
    "\n",
    "# 进行相关性比较\n",
    "model.similarity('dogs','you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min-count \n",
    "##### 在不同的语料集中，我们对于基准词频的需求也是不一样的，譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制，一般而言，合理的参数值设置在0-100之间\n",
    "\n",
    "### size\n",
    "##### size参数主要是用来设置神经网络的层数的，word2vec的默认值设置是设置为100层，更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度合理的设置范围为10~数百"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
