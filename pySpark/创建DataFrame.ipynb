{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=my app, master=local) created by __init__ at <ipython-input-3-cf928bf41a47>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cf928bf41a47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my app'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m stringJSONRDD = sc.parallelize((\"\"\" \n\u001b[0;32m      5\u001b[0m   { \"id\": \"123\",\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    306\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 308\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    309\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=my app, master=local) created by __init__ at <ipython-input-3-cf928bf41a47>:3 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('my app')\n",
    "sc = SparkContext(conf=conf)\n",
    "stringJSONRDD = sc.parallelize((\"\"\"\n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建临时表\n",
    "swimmersJSON.createOrReplaceTempView('swimmersJsontable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|concat(bro, wn)|\n",
      "+---------------+\n",
      "|          brown|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 编写sql查询\n",
    "spark.sql(\"select 'bro'||'wn' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from swimmersJsontable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from swimmersJsontable').collect()  # 返回所有行   不建议使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用反射来推断模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.printSchema()  # 打印出模式定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编程指定模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_train,fraud_test =swimmersJSON.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----+\n",
      "|age|eyeColor| id| name|\n",
      "+---+--------+---+-----+\n",
      "| 19|   brown|123|Katie|\n",
      "+---+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#生成以逗号分隔的数据\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'),( None, 'Michael', 22, 'grbeen'),(235, 'Michael', 22, 'grbeen'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  StructField被分解为一下方面\n",
    "# · Name 该字段的名字\n",
    "# . dataType 该字段的数据类型\n",
    "# . nullable 该字段的值是否为空\n",
    "schema = StructType([\n",
    "    StructField('id',LongType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('age',LongType(),True),\n",
    "    StructField('eyeColor',StringType(),True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后为我们的RDD应用该模式，并创建DataFrame\n",
    "swimmers = spark.createDataFrame(data=stringCSVRDD,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用dataframe创建一个临时视图\n",
    "swimmers.createOrReplaceTempView('swimmers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'swimmers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ce2a04d1be57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  collect（） show() take()  show take包含了限制返回行数的选项\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#行数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mswimmers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mswimmers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfn\u001b[0m  \u001b[1;31m#  导入所有的函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'swimmers' is not defined"
     ]
    }
   ],
   "source": [
    "### 利用dataframeAPI查询\n",
    "#  collect（） show() take()  show take包含了限制返回行数的选项\n",
    "#行数\n",
    "print(swimmers.count())\n",
    "swimmers.show()\n",
    "import pyspark.sql.functions as fn  #  导入所有的函数\n",
    "\n",
    "swimmers.agg(\n",
    "    fn.count('id').alias('id_count')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+-------+\n",
      "|  id|   name|age|eyeColor|income2|\n",
      "+----+-------+---+--------+-------+\n",
      "| 123|  Katie| 19|   brown|   2123|\n",
      "|null|Michael| 22|  grbeen|   null|\n",
      "| 235|Michael| 22|  grbeen|   2235|\n",
      "| 234|Michael| 22|   green|   2234|\n",
      "| 345| Simone| 23|    blue|   2345|\n",
      "+----+-------+---+--------+-------+\n",
      "\n",
      "+----+-------+---+--------+-------+-----+\n",
      "|  id|   name|age|eyeColor|income2|label|\n",
      "+----+-------+---+--------+-------+-----+\n",
      "| 123|  Katie| 19|   brown|   2123|    1|\n",
      "|null|Michael| 22|  grbeen|   null|    0|\n",
      "| 235|Michael| 22|  grbeen|   2235|    0|\n",
      "| 234|Michael| 22|   green|   2234|    0|\n",
      "| 345| Simone| 23|    blue|   2345|    0|\n",
      "+----+-------+---+--------+-------+-----+\n",
      "\n",
      "+---+-------+---+--------+-------+-----+\n",
      "| id|   name|age|eyeColor|income2|label|\n",
      "+---+-------+---+--------+-------+-----+\n",
      "|123|  Katie| 19|   brown|   2123|    1|\n",
      "|235|Michael| 22|  grbeen|   2235|    0|\n",
      "|234|Michael| 22|   green|   2234|    0|\n",
      "|345| Simone| 23|    blue|   2345|    0|\n",
      "+---+-------+---+--------+-------+-----+\n",
      "\n",
      "+----+-------+---+--------+-------+-----+----------+\n",
      "|  id|   name|age|eyeColor|income2|label|   thedate|\n",
      "+----+-------+---+--------+-------+-----+----------+\n",
      "| 123|  Katie| 19|   brown|   2123|    1|2018-04-11|\n",
      "|null|Michael| 22|  grbeen|   null|    0|2018-04-11|\n",
      "| 235|Michael| 22|  grbeen|   2235|    0|2018-04-11|\n",
      "| 234|Michael| 22|   green|   2234|    0|2018-04-11|\n",
      "| 345| Simone| 23|    blue|   2345|    0|2018-04-11|\n",
      "+----+-------+---+--------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1 = swimmers.withColumn(\"income2\",swimmers.id + 2000)\n",
    "test1.show()\n",
    "#2.给test1增加一列数据'label',当gender=='M'时,label=1,否则label=0.\n",
    "test2 = test1.withColumn(\"label\",fn.when(swimmers.age <=19,1).otherwise(0))\n",
    "test2.show() \n",
    "test2.filter(test2.id>1).show()\n",
    "#3.给test2增加一列数据'thedate',其值固定为'2018-04-11'\n",
    "test3 = test2.withColumn(\"thedate\",fn.lit('2018-04-11'))\n",
    "test3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+-------+-----+----------+-------+\n",
      "|  id|age|eyeColor|income2|label|   thedate|  name1|\n",
      "+----+---+--------+-------+-----+----------+-------+\n",
      "| 123| 19|   brown|   2123|    1|2018-04-11|  Katie|\n",
      "|null| 22|  grbeen|   null|    0|2018-04-11|Michael|\n",
      "+----+---+--------+-------+-----+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 重命名已存在的列\n",
    "test3.withColumnRenamed('name', 'name1').select([c for c in  test3.columns if c!='name' ]+['name1']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- income2: long (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      " |-- thedate: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "|sum(id)|min(age)|max(label)|\n",
      "+-------+--------+----------+\n",
      "|    937|      19|         1|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " test3.agg(fn.sum('id'),fn.min('age'),fn.max('label')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|   a|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "| 235|Michael|\n",
      "| 234|Michael|\n",
      "| 345| Simone|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选语句\n",
    "# 获得 age=22的id\n",
    "swimmers.select(fn.col('id').alias('a') ,'name').filter('age>=22').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "|345| 23|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 另一种方法\n",
    "swimmers.select(swimmers.id,swimmers.age).filter(swimmers.age>=22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "| Katie|\n",
      "|Simone|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果想获得眼睛颜色以 b开头的名字\n",
    "swimmers.select(swimmers.name).filter(\"eyecolor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|sum1|\n",
      "+----+\n",
      "|   3|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 利用sql查询\n",
    "# 行数\n",
    "spark.sql(\"select count(*) as sum1 from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|123| 19|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用where子句\n",
    "spark.sql(\"select id,age from swimmers where age=19 and name='Katie'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n",
      "| id| name|age|eyeColor|\n",
      "+---+-----+---+--------+\n",
      "|123|Katie| 19|   brown|\n",
      "+---+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查眼镜颜色以b开头的\n",
    "spark.sql(\"select * from swimmers where eyeColor like 'b%' limit 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 csv数据源\n",
    "\n",
    "flightPerfFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\n",
    "airportsFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset \n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'age'), (1, 'gender'), (2, 'id'), (3, 'username')]\n"
     ]
    }
   ],
   "source": [
    "# 为列表生成索引\n",
    "col_list = ['username','id','gender','age']\n",
    "mapping_list = list(enumerate(sorted(col_list)))\n",
    "print(mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2, 'gender': 1, 'username': 3, 'age': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#将mapping_list中的key和value互换位置,并转换为dict\n",
    "revs_maplist = {value:idx for [idx,value] in mapping_list}\n",
    "print(revs_maplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, -3, 10, -5, 0, 10.5]\n",
      "[1, 2, 0, 10, 0, 0, 10.5]\n"
     ]
    }
   ],
   "source": [
    "test_list = [1,2,-3,10,None,-5,0,10.5] \n",
    "#for循环简写1 (此处if在for循环后面)\n",
    "result1 = [item  for item in test_list if item != None]\n",
    "print(result1) \n",
    "#for循环简写2 (此处if-else必须同时存在且在for循环前面)\n",
    "result2  = [item if item > 0 else 0 for item in result1]\n",
    "print(result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize([(1, 2, 3, 'a b c'),\n",
    "                     (4, 5, 6, 'd e f'),\n",
    "                     (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   2|   3|   a|\n",
      "|   1|   2|   3|   b|\n",
      "|   1|   2|   3|   c|\n",
      "|   4|   5|   6|   d|\n",
      "|   4|   5|   6|   e|\n",
      "|   4|   5|   6|   f|\n",
      "|   7|   8|   9|   g|\n",
      "|   7|   8|   9|   h|\n",
      "|   7|   8|   9|   i|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn('col4',explode(split('col4',' '))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     r|\n",
      "+------+\n",
      "|[1, b]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df = sc.parallelize([Row(r=Row(a=1, b=\"b\"))]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|r.b|\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.r.getField(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
    "# [Row(ages=u'2'), Row(ages=u'5')]\n",
    "df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
    "# [Row(ages=u'2'), Row(ages=u'5')]\n",
    "# 将字符串转为 int 型\n",
    "df.select(df.source.cast(\"int\").alias('sources')).take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x26cde682208>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x26cde682128>)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result=rdd.groupBy(lambda x:x%2)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [2, 8]), (1, [1, 1, 3, 5])]\n"
     ]
    }
   ],
   "source": [
    "resultGp=[(x,sorted(y)) for (x,y) in result.collect()]\n",
    "print(resultGp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 1), ('a', 1)]\n",
      "[('a', <pyspark.resultiterable.ResultIterable object at 0x0000026CDE638748>), ('b', <pyspark.resultiterable.ResultIterable object at 0x0000026CDE6382E8>)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rddGp=rdd.groupByKey()\n",
    "print(rdd.collect())\n",
    "print(rddGp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[('a', [1, 1]), ('b', [1])]\n",
      "[('a', [2, 2]), ('b', [2])]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    a=list(x)#直接使用x会报错，说明sequence并不能用for\n",
    "    for i in range(len(a)):\n",
    "        a[i]=a[i]*2\n",
    "    return a\n",
    "gpMp1=rddGp.mapValues(len)\n",
    "gpMp2=rddGp.mapValues(list)\n",
    "gpMp3=rddGp.mapValues(f)\n",
    "print(gpMp1.collect())\n",
    "print(gpMp2.collect())\n",
    "print(gpMp3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='20150498')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df=spark.createDataFrame([('2015-04-08',)],['a'])\n",
    "df.select(fn.date_format('a','YYYYMMDD').alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|summary|         a|\n",
      "+-------+----------+\n",
      "|  count|         1|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|2015-04-08|\n",
      "|    max|2015-04-08|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-144-bd66ec2c06a6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-144-bd66ec2c06a6>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    ‪\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#将数组或者矩阵存储为csv文件可以使用如下代码实现：\n",
    "#numpy.savetxt('new.csv', my_matrix, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8  3.   1.4  0.1]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 6.7  3.1  4.7  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "csv_file= np.loadtxt(open(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\predict_dc.csv\",encoding='utf-8'),dtype=np.str,delimiter=',',unpack=False)\n",
    "\n",
    "data = csv_file[1:,0:].astype(np.float)\n",
    "\n",
    "print(data)\n",
    "np.savetxt('C:\\\\Users\\\\Administrator\\\\Desktop\\\\predict_dc111.csv', data, delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三种join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice| 18|\n",
      "|  2|  Andy| 19|\n",
      "|  3|   Bob| 17|\n",
      "|  4|Justin| 21|\n",
      "|  5| Cindy| 20|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,'Alice', 18),(2,'Andy', 19),(3,'Bob', 17),(4,'Justin', 21),(5,'Cindy', 20)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|Alice|   160|\n",
      "| Andy|   159|\n",
      "|  Bob|   170|\n",
      "|Cindy|   165|\n",
      "| Rose|   160|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([('Alice', 160),('Andy', 159),('Bob', 170),('Cindy', 165),('Rose', 160)])\n",
    "schema2 = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "df2 = spark.createDataFrame(rdd2, schema2)\n",
    "df2.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|height|\n",
      "+---+------+------+\n",
      "|  1| Alice|   160|\n",
      "|  2|  Andy|   159|\n",
      "|  3|   Tom|   175|\n",
      "|  4|Justin|   171|\n",
      "|  5| Cindy|   165|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#创建第三个dataframe\n",
    "rdd3 = sc.parallelize([(1,'Alice', 160),(2,'Andy', 159),(3,'Tom', 175),(4,'Justin', 171),(5,'Cindy', 165)])\n",
    "schema3 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "df3 = spark.createDataFrame(rdd3, schema3)\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|height|\n",
      "+---+-----+---+------+\n",
      "|  1|Alice| 18|   160|\n",
      "|  2| Andy| 19|   159|\n",
      "|  3|  Bob| 17|   170|\n",
      "|  5|Cindy| 20|   165|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### inner join\n",
    "df.join(df2,'name','inner').select('id',df.name,'age','height').orderBy('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|height|\n",
      "+---+-----+---+------+\n",
      "|  5|Cindy| 20|   165|\n",
      "|  3|  Bob| 17|   170|\n",
      "|  2| Andy| 19|   159|\n",
      "|  1|Alice| 18|   160|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2,'name','inner').select('id',df.name,'age','height').orderBy('id',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 不写连接类型默认是inner\n",
    "df.join(df2,'name','inner').select('id',df.name,'age','height').orderBy('id',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果内连接的参数不只一个，将参数放在一个列表中\n",
    "df.join(df3, [df.id==df3.id, df.name==df3.name], \"inner\").select(df.id).orderBy(df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------+\n",
      "| id|  name| age|height|\n",
      "+---+------+----+------+\n",
      "|  4|  Rose|null|   160|\n",
      "|  1| Alice|  18|   160|\n",
      "|  2|  Andy|  19|   159|\n",
      "|  3|   Bob|  17|   170|\n",
      "|  4|Justin|  21|   170|\n",
      "|  5| Cindy|  20|   165|\n",
      "+---+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outer join \n",
    "# full outer join全外连接 \n",
    "# 注意：不能用…select(df.name)，会报错\n",
    "\n",
    "aa = df.join(df2, \"name\", \"outer\").select(\"id\", \"name\", \"age\", \"height\").orderBy(\"id\")\n",
    "means={'id': 4.0,'height':170}\n",
    "aa.fillna(means).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|height|\n",
      "+---+------+---+------+\n",
      "|  1| Alice| 18|   160|\n",
      "|  2|  Andy| 19|   159|\n",
      "|  3|   Bob| 17|   170|\n",
      "|  4|Justin| 21|  null|\n",
      "|  5| Cindy| 20|   165|\n",
      "+---+------+---+------+\n",
      "\n",
      "+---+------+---+------+\n",
      "| id|  name|age|height|\n",
      "+---+------+---+------+\n",
      "|  1| Alice| 18|   160|\n",
      "|  2|  Andy| 19|   159|\n",
      "|  3|   Bob| 17|   170|\n",
      "|  4|Justin| 21|  null|\n",
      "|  5| Cindy| 20|   165|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#left outer join\n",
    "\n",
    "df.join(df2, \"name\", \"left\").select(\"id\", \"name\", \"age\", \"height\").orderBy(\"id\").show()\n",
    "# 或者\n",
    "df.join(df2, \"name\", \"left\").select(\"id\", df.name, \"age\", \"height\").orderBy(\"id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+------+\n",
      "|  id| name| age|height|\n",
      "+----+-----+----+------+\n",
      "|null| Rose|null|   160|\n",
      "|   1|Alice|  18|   160|\n",
      "|   2| Andy|  19|   159|\n",
      "|   3|  Bob|  17|   170|\n",
      "|   5|Cindy|  20|   165|\n",
      "+----+-----+----+------+\n",
      "\n",
      "+----+-----+----+------+\n",
      "|  id| name| age|height|\n",
      "+----+-----+----+------+\n",
      "|null| Rose|null|   160|\n",
      "|   1|Alice|  18|   160|\n",
      "|   2| Andy|  19|   159|\n",
      "|   3|  Bob|  17|   170|\n",
      "|   5|Cindy|  20|   165|\n",
      "+----+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# right outer join\n",
    "\n",
    "df.join(df2, \"name\", \"right\").select(\"id\", \"name\", \"age\", \"height\").orderBy(\"id\").show()\n",
    "# 或者\n",
    "df.join(df2, \"name\", \"right\").select(\"id\", df2.name, \"age\", \"height\").orderBy(\"id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "| Andy|   159|\n",
      "| Rose|   160|\n",
      "|Alice|   160|\n",
      "|Cindy|   165|\n",
      "|  Bob|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.orderBy(\"height\",fn.desc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "| Andy|   159|\n",
      "|Alice|   160|\n",
      "| Rose|   160|\n",
      "|Cindy|   165|\n",
      "|  Bob|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.orderBy([\"height\",\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 18|\n",
      "| 19|\n",
      "| 17|\n",
      "| 21|\n",
      "| 20|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cast()函数 \n",
    "#常用来做类型转换\n",
    "\n",
    "df.select(df.age.cast(StringType())).show()\n",
    "# 或者\n",
    "df.selectExpr(\"cast(age as string)age\").prinSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##row_number().over()\n",
    "from pyspark.sql import Window\n",
    "rdd = sc.parallelize([(1,'Alice', 18),(2,'Andy', 19),(3,'Bob', 17),(1,'Justin', 21),(1,'Cindy', 20)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+\n",
      "| id|  name|age| rn|\n",
      "+---+------+---+---+\n",
      "|  1| Alice| 18|  1|\n",
      "|  1| Cindy| 20|  2|\n",
      "|  1|Justin| 21|  3|\n",
      "|  3|   Bob| 17|  1|\n",
      "|  2|  Andy| 19|  1|\n",
      "+---+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "#按照每个组内的年龄排序，组外的分布并不管\n",
    "\n",
    "a =df.withColumn(\"rn\", fn.row_number().over(Window.partitionBy(\"id\").orderBy(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+\n",
      "| id|  name|age| rn|\n",
      "+---+------+---+---+\n",
      "|  1| Alice| 18|  1|\n",
      "|  1| Cindy| 20|  2|\n",
      "|  1|Justin| 21|  3|\n",
      "|  3|   Bob| 17|  1|\n",
      "|  2|  Andy| 19|  1|\n",
      "+---+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#按照年龄排序，组外面分布也管\n",
    "df.withColumn(\"rn\", fn.row_number().over(Window.partitionBy(\"id\").orderBy(\"age\"))).orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    3|\n",
      "|  3|    1|\n",
      "|  2|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 18|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.like('Al%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     v|\n",
      "+------+\n",
      "|1.0000|\n",
      "|2.0000|\n",
      "|3.0000|\n",
      "|1.0000|\n",
      "|1.0000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(fn.format_number('id',4).alias('v')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "rdd = sc.parallelize([('牛奶+进口','12215,9434,13604'),('除湿','15932,1283,1661'),('除湿','15932,1283,1661'),('除湿','15932,1283,1661')])\n",
    "schema = StructType([\n",
    "    StructField(\"key_word\", StringType(), True),\n",
    "    StructField(\"high_value\", StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd3 = sc.parallelize([(3998,'15932'),(6909,'8302'),(4565,'12215'),(12345,'1661')])\n",
    "schema3 = StructType([\n",
    "    StructField(\"brand_cd\", IntegerType(), True),\n",
    "    StructField(\"item\", StringType(), True)\n",
    "])\n",
    "df3 = spark.createDataFrame(rdd3, schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------+---+\n",
      "|brand_cd|key_word|chaxunliang| rn|\n",
      "+--------+--------+-----------+---+\n",
      "|   12345|      除湿|          3|  1|\n",
      "|    4565|   牛奶+进口|          1|  1|\n",
      "|    3998|      除湿|          3|  1|\n",
      "+--------+--------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('guanjianci')\n",
    "df3.createOrReplaceTempView('pinpaibiao')\n",
    "spark.sql(\n",
    "    '''\n",
    "select brand_cd, key_word,chaxunliang,rn from \n",
    "(\n",
    "select   brand_cd, key_word,chaxunliang,row_number() over(partition by  brand_cd, key_word  order by chaxunliang desc) as rn  from\n",
    "(\n",
    "select brand_cd, key_word ,count(*) as chaxunliang  from  guanjianci a   join pinpaibiao b  on ','||a.high_value||','  like '%,' || b.item || ',%'\n",
    "    group by 1,2 ) a)b where rn<=20\n",
    "    '''\n",
    ").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
