{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('my app')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combineCtrs(c1,c2):\n",
    "    return (c1[0]+c2[0],c1[1]+c2[1])\n",
    "\n",
    "def basicAVG(nums):\n",
    "    return nums.map(lambda num:(num,1)).reduce(combineCtrs)\n",
    "    \n",
    "nums = sc.parallelize([1,2,3,4,2,3,4,5])\n",
    "# basicAVG(nums).collect()\n",
    "nums.map(lambda num:(num,1)).reduce(combineCtrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 11, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"<ipython-input-14-9c12ff70eaca>\", line 5, in partitionCtr\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"<ipython-input-14-9c12ff70eaca>\", line 5, in partitionCtr\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9c12ff70eaca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mnums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mfastAVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9c12ff70eaca>\u001b[0m in \u001b[0;36mfastAVG\u001b[1;34m(nums)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfastAVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msumcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionCtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombineCtrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msumcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msumCount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \"\"\"\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 11, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"<ipython-input-14-9c12ff70eaca>\", line 5, in partitionCtr\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 230, in main\n  File \"d:\\Python\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 225, in process\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"d:\\python\\lib\\site-packages\\pyspark\\rdd.py\", line 370, in func\n    return f(iterator)\n  File \"<ipython-input-14-9c12ff70eaca>\", line 5, in partitionCtr\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "def partitionCtr(nums):\n",
    "    '''计算分区的sumCounter'''\n",
    "    sumcount = [0,0]\n",
    "    for num in nums:\n",
    "        sumcount[0] +=num[0]\n",
    "        sumcount[1] +=num[1]\n",
    "    return sumcount\n",
    "\n",
    "def fastAVG(nums):\n",
    "    sumcount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
    "    return float(sumcount[0])/float(sumCount[1])\n",
    "\n",
    "nums = sc.parallelize([1,2,3,4,2,3,4,5])\n",
    "fastAVG(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0, 2.0, 3.0, 4.0, 5.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "### 用python移除异常值\n",
    "### 要把String类型的RDD 转换为数字数据，这样才能使用统计函数并移除异常值\n",
    "nums = sc.parallelize([1,2,3,4,2,3,4,5])\n",
    "distanceNumerics = nums.map(lambda x:float(x))\n",
    "stats = distanceNumerics.stats()\n",
    "stddev = stats.stdev()   # 标准差\n",
    "mean = stats.mean()    # 平均值\n",
    "reasonableDistance = distanceNumerics.filter( lambda x: math.fabs(x-mean)<3*stddev)\n",
    "print( reasonableDistance.collect())\n",
    "stats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 合并过多的分区\n",
    "input = sc.textFile(r'C:\\\\Users\\\\Administrator\\\\Desktop\\\\*.txt')\n",
    "input.getNumPartitions()\n",
    "### 排除掉大部分数据的筛选方法\n",
    "lines = input.filter(lambda line : line.startswith('一个'))\n",
    "lines.getNumPartitions()\n",
    "\n",
    "### 在缓存lines之前先对其进行合并操作\n",
    "lines = lines.coalesce(2).cache()\n",
    "lines.getNumPartitions()\n",
    "### 然后对合并后的RDD操作\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "44.14000000000001\n",
      "[('iphone', 2.0), ('ipad', 1.0), ('head', 1.0), ('samsung', 1.0)]\n",
      "iphone 2.0\n"
     ]
    }
   ],
   "source": [
    "### 编写简单的应用\n",
    "#from pyspark import SparkConf,SparkContext\n",
    "#conf = SparkConf().setMaster('local').setAppName('my app')\n",
    "#sc = SparkContext(conf=conf)\n",
    "data = sc.parallelize((('j,iphone,9.9'),('j,head,9.9'),('ja,iphone,9.9'),('jill,samsung,8.95'),('bob,ipad,5.49')))\n",
    "# 将数据转化为（ user,product,price）格式\n",
    "# data = sc.textFile('路径')\n",
    "\n",
    "lines =  data.map(lambda line:line.split(',')).map(lambda record:(record[0],record[1],record[2]))\n",
    "# 求购买次数\n",
    "numPurchases = lines.count()\n",
    "# 求有多少不同客户购买过商品\n",
    "uniqueUsers = lines.map(lambda record:record[0]).distinct().count()\n",
    "# 求和的出总收入\n",
    "totalRevenue = lines.map(lambda record: float(record[2])).sum()\n",
    "# 求最畅销的商品\n",
    "products = lines.map(lambda record:(record[1],1.0)).reduceByKey(lambda a,b :a+b).collect()\n",
    "mostPopul  = sorted(products,key = lambda x:x[1],reverse=True)[0]\n",
    "print(numPurchases)\n",
    "print(uniqueUsers)\n",
    "print(totalRevenue)\n",
    "print(products)\n",
    "print(mostPopul[0],mostPopul[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movie_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-be560b0fad9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1999\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmovie_fields\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mmovie_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0myesrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_fields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mconvert_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 将解析出的过滤掉\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'movie_data' is not defined"
     ]
    }
   ],
   "source": [
    "### 数据清理\n",
    "\n",
    "def convert_year(x):\n",
    "    try :\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1999\n",
    "movie_fields=  movie_data.map(lambda lines:lines.split(\"|\"))\n",
    "yesrs = movie_fields.map(lambda fields:fields[2]).map(lambda x:convert_year(x))\n",
    "# 将解析出的过滤掉\n",
    "year_filtered = years.filter(lambda x:x!=1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afternoon', 'evening', 'afternoon', 'evening', 'night']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_tod(hr):\n",
    "    times_of_day={\n",
    "        'morning':range(7,12),\n",
    "        'lunch':range(12,14),\n",
    "        'afternoon':range(14,18),\n",
    "        'evening':range(18,23),\n",
    "        'night':[23,24]+[1,2,3,4,5,6]\n",
    "        \n",
    "    }\n",
    "    for k,v in times_of_day.items():\n",
    "        if hr in v:\n",
    "            return k\n",
    "nums = sc.parallelize([17,21,15,18,6])\n",
    "time =  nums.map(lambda hr:assign_tod(hr))\n",
    "time.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M')], \n",
    "                           ['id', \n",
    "                            'weight', \n",
    "                            'height', \n",
    "                            'age', \n",
    "                            'gender']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 7\n",
      "Count of distinct rows: 6\n"
     ]
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows: {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of ids:6\n",
      "count of distinct ids:5\n"
     ]
    }
   ],
   "source": [
    "# 除了ID 字段外的重复行\n",
    "print('count of ids:{0}'.format(df.count()) )\n",
    "print('count of distinct ids:{0}'.format(df.select([c for c in df.columns if c!='id']).distinct().count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ids: 6\n",
      "Count of distinct ids:5\n"
     ]
    }
   ],
   "source": [
    "# 或者\n",
    "print('Count of ids: {0}'.format(df.count()))\n",
    "\n",
    "\n",
    "#print('Count of distinct ids: {0}'.format(df.select([c for c in df.columns if c != 'id']).distinct().count()))\n",
    "print('Count of distinct ids:{0}'.format(df.select([\"weight\",\"height\",\"age\",\"gender\"]).distinct().count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 继续删除重复项 除了id   使用 .dropDuplicates(...) 处理重复行，但是添加 subset 参数\n",
    "df = df.dropDuplicates([c for c in df.columns if c !='id'])\n",
    "# 或者    df =df.dropDuplicates([\"weight\",\"height\",\"age\",\"gender\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|id_count|id_distinct|\n",
      "+--------+-----------+\n",
      "|       5|          4|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用.agg(...)函数计算ID的总数和ID唯一个数\n",
    "import pyspark.sql.functions as fn  #  导入所有的函数\n",
    "\n",
    "df.agg(\n",
    "    fn.count('id').alias('id_count'),\n",
    "    fn.countDistinct('id').alias('id_distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 给每一行一个唯一的ID号，相当于新增一列数字\n",
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()   # 提供了递增的且唯一的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss = spark.createDataFrame([\n",
    "        (1, 143.5, 5.6, 28,   'M',  100000),\n",
    "        (2, 167.2, 5.4, 45,   'M',  None),\n",
    "        (3, None , 5.2, None, None, None),\n",
    "        (4, 144.5, 5.9, 33,   'M',  None),\n",
    "        (5, 133.2, 5.7, 54,   'F',  None),\n",
    "        (6, 124.1, 5.2, None, 'F',  None),\n",
    "        (7, 129.2, 5.3, 42,   'M',  76000),\n",
    "    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])\n",
    "\n",
    "df_miss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(income)|\n",
      "+-------------+\n",
      "|            2|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 统计每行缺失值的个数\n",
    "df_miss.rdd.map( lambda row: (sum([c==None for c in row ]),row['id']) ).sortByKey(ascending=False).collect()\n",
    "df_miss.agg(*[fn.count('income')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "+---+------+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss.where('id = 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 每一列缺失值数据的百分比\n",
    "# fn.count(c) / fn.count('*')  为每列字段非缺失的记录数占比\n",
    "\n",
    "# fn.count('*')  *表示列名的位置，指示该列方法计算所有的列\n",
    "# .agg(*   *之前的列指示，.agg(...)方法将该列表处理为一组独立的参数传递给函数\n",
    "\n",
    "df_miss.agg(*[\n",
    "    (1-(fn.count(c)/fn.count('*'))).alias(c+'_missing') for c in df_miss.columns\n",
    "]\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 移除income列，因为income列大部分是缺失值，缺失值占比达到71%\n",
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c!='income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  3|  null|   5.2|null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 如果想要移除id为3的行，我们可以使用.dropna(...)方法，\n",
    "#在这里，我们利用thresh参数，该参数允许我们为每一行点的缺失观测数据的数量指定一个阈值\n",
    "df_miss_no_income.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 40.399999999999999, 'weight': 140.28333333333333, 'height': 5.4714285714285706, 'id': 4.0}\n",
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             143.5|   5.6| 28|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.28333333333333|   5.2| 40|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 40|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 使用.fillna(...) 方法，填充观测数据. 可以填充单个整形，浮点型，字符串   也可以是字典\n",
    "### 我们将agg的输出转化为 pandas的dataframe 在次传递一个代码字典  \n",
    "### pandas的to_dict(...)方法的records参数指示他创建代码字典\n",
    "means = df_miss_no_income.agg(\n",
    "    *[\n",
    "        fn.mean(c).alias(c) for c in df.columns if c!='gender'\n",
    "    ]\n",
    ").toPandas().to_dict('records')[0]\n",
    "print(means)\n",
    "\n",
    "means['gender'] = 'missing'   #  我们忽略gender列因为我们计算不出一个类别\n",
    "df_miss_no_income.fillna(means).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  3| 342.3|   5.1| 99|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  7| 129.2|   5.3| 42|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 离群值 指那些与样本其余部分的分布显著偏离的观测数据\n",
    "df_outliers = spark.createDataFrame([\n",
    "        (1, 143.5, 5.3, 28),\n",
    "        (2, 154.2, 5.5, 45),\n",
    "        (3, 342.3, 5.1, 99),\n",
    "        (4, 144.5, 5.5, 33),\n",
    "        (5, 133.2, 5.4, 54),\n",
    "        (6, 124.1, 5.1, 21),\n",
    "        (7, 129.2, 5.3, 42),\n",
    "    ], ['id', 'weight', 'height', 'age'])\n",
    "df_outliers.show()\n",
    "import numpy as np\n",
    "np.array(df_outliers.toPandas())\n",
    "df_p = df_outliers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n",
      "| id|weight|height|age|\n",
      "+---+------+------+---+\n",
      "|  1| 143.5|   5.3| 28|\n",
      "|  2| 154.2|   5.5| 45|\n",
      "|  3| 342.3|   5.1| 99|\n",
      "|  4| 144.5|   5.5| 33|\n",
      "|  5| 133.2|   5.4| 54|\n",
      "|  6| 124.1|   5.1| 21|\n",
      "|  7| 129.2|   5.3| 42|\n",
      "+---+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df_p = pd.DataFrame(dict(num=range(3),char=['a','b','c']))\n",
    "df_s = spark.createDataFrame(df_p) # pandas dataframe转化成PySpark DataFrame\n",
    "type(df_s)\n",
    "df_s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129.2, 154.2]\n",
      "{'weight': [91.69999999999999, 191.7]}\n",
      "[5.1, 5.5]\n",
      "{'weight': [91.69999999999999, 191.7], 'height': [4.499999999999999, 6.1000000000000005]}\n",
      "[28.0, 54.0]\n",
      "{'age': [-11.0, 93.0], 'weight': [91.69999999999999, 191.7], 'height': [4.499999999999999, 6.1000000000000005]}\n"
     ]
    }
   ],
   "source": [
    "# 使用 .approxQuantile(...) 方法   approxQuantile(col, probabilities, relativeError) New in version 2.0\n",
    "# 第一个参数是列名 第二个参数值 0-1之间的其中一个数（其中0.5是指计算的中位数）或者一个列表（在这个例子中），\n",
    "# 第三个参数是指定每个度量的一个可接受的错误程度（如果设置为0，就会计算一个度量的准确值，但是代价很大）\n",
    "cols = ['weight','height','age']\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col,[0.25,0.75],0.05)\n",
    "    IQR = quantiles[1]-quantiles[0]\n",
    "    bounds[col]=[\n",
    "        quantiles[0]-1.5*IQR,\n",
    "        quantiles[1]+1.5*IQR\n",
    "    ]\n",
    "    print(quantiles)\n",
    "    print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  7|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  1|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  2|   false|   false|false|\n",
      "|  4|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 现在用bround来标记我们的离群值\n",
    "outliers = df_outliers.select(\n",
    "    ['id']+[((df_outliers[c]<bounds[c][0])|(df_outliers[c]>bounds[c][1])).alias(c+'_o') for c in cols ]  \n",
    ")\n",
    "outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+--------+--------+-----+\n",
      "| id|weight|height|age|weight_o|height_o|age_o|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "|  7| 129.2|   5.3| 42|   false|   false|false|\n",
      "|  6| 124.1|   5.1| 21|   false|   false|false|\n",
      "|  5| 133.2|   5.4| 54|   false|   false|false|\n",
      "|  1| 143.5|   5.3| 28|   false|   false|false|\n",
      "|  3| 342.3|   5.1| 99|    true|   false| true|\n",
      "|  2| 154.2|   5.5| 45|   false|   false|false|\n",
      "|  4| 144.5|   5.5| 33|   false|   false|false|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列出weight和age与其他分部明显不同的部分(离群值)\n",
    "df_outliers = df_outliers.join(outliers,on='id')\n",
    "df_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|weight|\n",
      "+---+------+\n",
      "|  7| 129.2|\n",
      "|  6| 124.1|\n",
      "|  5| 133.2|\n",
      "|  1| 143.5|\n",
      "|  2| 154.2|\n",
      "|  4| 144.5|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.filter(df_outliers.weight_o==False).select('id','weight').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 99|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.filter('age_o').select('id','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过上述方法即可获得快速清洗数据（重复值、缺失值和离群值）\n",
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fraud = sc.textFile('D:\\\\Downloads\\\\ccFraud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "header  = fraud.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将头去掉\n",
    "fraud =fraud.filter(lambda row: row!=header).map(lambda row:[int(elem) for elem in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(custID,IntegerType,true),\n",
       " StructField(gender,IntegerType,true),\n",
       " StructField(state,IntegerType,true),\n",
       " StructField(cardholder,IntegerType,true),\n",
       " StructField(balance,IntegerType,true),\n",
       " StructField(numTrans,IntegerType,true),\n",
       " StructField(numIntlTrans,IntegerType,true),\n",
       " StructField(creditLine,IntegerType,true),\n",
       " StructField(fraudRisk,IntegerType,true)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们创建自己的DataFrame  h[1:-1]代表第一行到最后一行\n",
    "fields=[\n",
    "    *[\n",
    "        typ.StructField(h[1:-1],typ.IntegerType(),True) for h in header.split(',')\n",
    "    ]\n",
    "]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType(fields)\n",
    "# 最后创建我们的dataFrame\n",
    "fraud_df = spark.createDataFrame(fraud,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- custID: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- state: integer (nullable = true)\n",
      " |-- cardholder: integer (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- numTrans: integer (nullable = true)\n",
      " |-- numIntlTrans: integer (nullable = true)\n",
      " |-- creditLine: integer (nullable = true)\n",
      " |-- fraudRisk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "|     1|     1|   35|         1|   3000|       4|          14|         2|        0|\n",
      "|     2|     2|    2|         1|      0|       9|           0|        18|        0|\n",
      "|     3|     2|    2|         1|      0|      27|           9|        16|        0|\n",
      "|     4|     1|   15|         1|      0|      12|           0|         5|        0|\n",
      "|     5|     1|   46|         1|      0|      11|          16|         7|        0|\n",
      "|     6|     2|   44|         2|   5546|      21|           0|        13|        0|\n",
      "|     7|     1|    3|         1|   2000|      41|           0|         1|        0|\n",
      "|     8|     1|   10|         1|   6016|      20|           3|         6|        0|\n",
      "|     9|     2|   32|         1|   2428|       4|          10|        22|        0|\n",
      "|    10|     1|   23|         1|      0|      18|          56|         5|        0|\n",
      "|    11|     1|   46|         1|   4601|      54|           0|         4|        0|\n",
      "|    12|     1|   10|         1|   3000|      20|           0|         2|        0|\n",
      "|    13|     1|    6|         1|      0|      45|           2|         4|        0|\n",
      "|    14|     2|   38|         1|   9000|      41|           3|         8|        0|\n",
      "|    15|     1|   27|         1|   5227|      60|           0|        17|        0|\n",
      "|    16|     1|   44|         1|      0|      22|           0|         5|        0|\n",
      "|    17|     2|   18|         1|  13970|      20|           0|        13|        0|\n",
      "|    18|     1|   35|         1|   3113|      13|           6|         8|        0|\n",
      "|    19|     1|    5|         1|   9000|      20|           2|         8|        0|\n",
      "|    20|     2|   31|         1|   1860|      21|          10|         8|        0|\n",
      "+------+------+-----+----------+-------+--------+------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     1|6178231|\n",
      "|     2|3821769|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为了更好的分类别，我们利用groupby方法计算这些列的使用频率\n",
    "fraud_df.groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|          balance|          numTrans|     numIntlTrans|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|         10000000|          10000000|         10000000|\n",
      "|   mean|     4109.9199193|        28.9351871|        4.0471899|\n",
      "| stddev|3996.847309737258|26.553781024523122|8.602970115863904|\n",
      "|    min|                0|                 0|                0|\n",
      "|    max|            41485|               100|               60|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#我们所面对的样本一个性别比例失衡的样本.使用describe()方法进行数值统计\n",
    "desc = fraud_df.describe(['balance', 'numTrans', 'numIntlTrans'])\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| skewness(balance)|\n",
      "+------------------+\n",
      "|1.1818315552993839|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#如何检测偏度\n",
    "fraud_df.agg({'balance':'skewness'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004452314017265386"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相关性 .corr()方法仅仅支持Pearson相关性系数，且两两相关性。\n",
    "#当然你可以把Spark DataFrame 转换为Python的DataFrame之后，你就可以随便怎么弄了，但是开销可能会很大，由数据量决定。期待后续版本的更新。\n",
    "fraud_df.corr('balance', 'numTrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.0004452314017265386, 0.0002713991339817875],\n",
       " [None, 1.0, -0.00028057128198165544],\n",
       " [None, None, 1.0]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建相关系数矩阵\n",
    "numerical = ['balance', 'numTrans', 'numIntlTrans']\n",
    "n_numerical = len(numerical)\n",
    "\n",
    "corr = []\n",
    "\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "    corr.append(temp)\n",
    "    \n",
    "corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# import bokeh.charts as chrt\n",
    "# from bokeh.io import output_notebook\n",
    "# output_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直方图Histograms\n",
    "## 方法一 聚集工作节点中的数据并返回一个汇总bins列表和直方图每个bin中的计数给驱动（适用大数据集）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(1) 先对数据进行聚合\n",
    "hists = fraud_df.select('balance').rdd.flatMap(lambda row: row).histogram(20)\n",
    "type(hists)\n",
    "tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22af309e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#使用matplotlib绘制直方图\n",
    "data = {\n",
    "    'bins': hists[0][:-1],\n",
    "    'freq': hists[1]\n",
    "}\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.bar(data['bins'], data['freq'], width=2000)\n",
    "ax.set_title('Histogram of balance')\n",
    "plt.savefig('balance.png', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-4f95214643b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 使用Bokeh绘制直方图\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mb_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'freq'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bins'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Histogram of balance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mchrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_hist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chrt' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用Bokeh绘制直方图\n",
    "b_hist = chrt.Bar(data, values='freq', label='bins', title='Histogram of balance')\n",
    "chrt.show(b_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+\n",
      "|balance|numTrans|numIntlTrans|\n",
      "+-------+--------+------------+\n",
      "|   8000|      14|           0|\n",
      "|   9000|      15|           0|\n",
      "|      0|     100|           0|\n",
      "|      0|       4|          11|\n",
      "|      0|      13|           0|\n",
      "|  11344|      32|          60|\n",
      "|   3074|      36|           6|\n",
      "|   1875|      19|           0|\n",
      "|    886|     100|          60|\n",
      "|  13135|      20|           4|\n",
      "|  14000|     100|           0|\n",
      "|   4367|      20|           0|\n",
      "|   4000|      11|           0|\n",
      "|   2270|      34|           2|\n",
      "|   7000|       9|           0|\n",
      "|   4000|      22|           0|\n",
      "|      0|      54|           0|\n",
      "|   3000|      35|           0|\n",
      "|   4000|      87|           0|\n",
      "|      0|       5|           0|\n",
      "+-------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#特征值之间的交互\n",
    "# 10000000条数据先抽样0.02%\n",
    "data_sample = fraud_df.sampleBy('gender', {1: 0.0002, 2: 0.0002}).select(numerical)\n",
    "data_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-34fe776e9596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msctr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_multi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'balance'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'numTrans'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mchrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msctr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chrt' is not defined"
     ]
    }
   ],
   "source": [
    "data_multi = dict([\n",
    "    (x, data_sample.select(x).rdd.flatMap(lambda row: row).collect()) \n",
    "    for x in numerical\n",
    "])\n",
    "\n",
    "sctr = chrt.Scatter(data_multi, x='balance', y='numTrans')\n",
    "\n",
    "chrt.show(sctr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 分割培训和测试数据\n",
    "fraud_train,fraud_test = fraud_df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
